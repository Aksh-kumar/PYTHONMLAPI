{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import copy\n",
    "from scipy.stats import multivariate_normal\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ln(exp^e1+expe2....)\n",
    "def ln_sum_exp(Z):\n",
    "    return np.log(np.sum(np.exp(Z)))\n",
    "\n",
    "def get_log_likelihood(data, weights, means, covs, num_clusters, num_dim):\n",
    "    #num_clusters = len(means)\n",
    "    #num_dim = len(data[0]) \n",
    "    sum_ln_exp = 0\n",
    "    for d in data:\n",
    "        Z = np.zeros(num_clusters)\n",
    "        for k in range(num_clusters):\n",
    "            # compute exponential term in multivariate_normal\n",
    "            delta = np.array(d) - means[k]\n",
    "            exponent_term = np.dot(delta.T, np.dot(np.linalg.inv(covs[k]), delta))\n",
    "            # Compute loglikelihood contribution for this data point and this cluster\n",
    "            Z[k] += np.log(weights[k])\n",
    "            Z[k] -= 1/2. * (num_dim * np.log(2*np.pi) + np.log(np.linalg.det(covs[k])) + exponent_term)\n",
    "        # Increment loglikelihood contribution of this data point across all clusters\n",
    "        sum_ln_exp += log_sum_exp(Z)\n",
    "    return sum_ln_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E step - get the responsibility if cluster parameter is given\n",
    "def get_responsibilities(data, weights, means, covariances):\n",
    "    n_data = len(data)\n",
    "    n_clusters = len(means)\n",
    "    resp = np.zeros((n_data, n_clusters))\n",
    "    for i in range(n_data):\n",
    "        for k in range(n_clusters):\n",
    "            resp[i, k] = weights[k]* multivariate_normal.pdf(data[i],means[k],covariances[k])\n",
    "    # Add up responsibilities over each data point and normalize\n",
    "    row_sums = resp.sum(axis=1)[:, np.newaxis]\n",
    "    resp = resp / row_sums\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M-step update the parameter if given cluster responsibilty is given\n",
    "# get sum of all responsibilty for a particular cluster\n",
    "# resp = nxk metrix i.e resp[number_of_label, nuber_of_cluster]\n",
    "get_soft_count = lambda resp : np.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of weights which represents how much each cluster represented over all data points\n",
    "# parameter count list of sum of soft counts for all clusters\n",
    "def get_weights(counts):\n",
    "    n_clusters = len(counts)\n",
    "    sum_count = np.sum(counts)\n",
    "    weights = np.array(list(map(lambda k : counts[k]/sum_count, range(n_clusters))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_means(data, resp, counts):\n",
    "    n_clusters = len(counts)\n",
    "    n_data = len(data)\n",
    "    means = np.zeros((len(data[0]), n_clusters))\n",
    "    for k in range(num_clusters):\n",
    "        weighted_sum = reduce(lambda x,i : x + resp[i,k]*data[i], range(n_data), 0)\n",
    "        means[k] = weighted_sum/counts[k]\n",
    "\n",
    "    return means"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
